digraph {
	graph [size="16.95,16.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2585547532064 [label="
 (1, 256)" fillcolor=darkolivegreen1]
	2585547510496 [label=LogSoftmaxBackward0]
	2585547510592 -> 2585547510496
	2585547510592 [label=AddmmBackward0]
	2585547510208 -> 2585547510592
	2585547530864 [label="act_fc1.bias
 (256)" fillcolor=lightblue]
	2585547530864 -> 2585547510208
	2585547510208 [label=AccumulateGrad]
	2585547510400 -> 2585547510592
	2585547510400 [label=ViewBackward0]
	2585547510640 -> 2585547510400
	2585547510640 [label=ReluBackward0]
	2585547510832 -> 2585547510640
	2585547510832 [label=ConvolutionBackward0]
	2585547510928 -> 2585547510832
	2585547510928 [label=ReluBackward0]
	2585547511120 -> 2585547510928
	2585547511120 [label=ConvolutionBackward0]
	2585547511216 -> 2585547511120
	2585547511216 [label=ReluBackward0]
	2585547511456 -> 2585547511216
	2585547511456 [label=ConvolutionBackward0]
	2585547511552 -> 2585547511456
	2585547511552 [label=ReluBackward0]
	2585547511744 -> 2585547511552
	2585547511744 [label=ConvolutionBackward0]
	2585547511840 -> 2585547511744
	2585547530944 [label="x
 (256, 4, 1, 1)" fillcolor=lightblue]
	2585547530944 -> 2585547511840
	2585547511840 [label=AccumulateGrad]
	2585547511792 -> 2585547511744
	2585547497040 [label="conv1.weight
 (32, 4, 3, 3)" fillcolor=lightblue]
	2585547497040 -> 2585547511792
	2585547511792 [label=AccumulateGrad]
	2585547511648 -> 2585547511744
	2585547497120 [label="conv1.bias
 (32)" fillcolor=lightblue]
	2585547497120 -> 2585547511648
	2585547511648 [label=AccumulateGrad]
	2585547511504 -> 2585547511456
	2585547497280 [label="conv2.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	2585547497280 -> 2585547511504
	2585547511504 [label=AccumulateGrad]
	2585547511360 -> 2585547511456
	2585547497360 [label="conv2.bias
 (64)" fillcolor=lightblue]
	2585547497360 -> 2585547511360
	2585547511360 [label=AccumulateGrad]
	2585547511168 -> 2585547511120
	2585547530384 [label="conv3.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	2585547530384 -> 2585547511168
	2585547511168 [label=AccumulateGrad]
	2585547511024 -> 2585547511120
	2585547530464 [label="conv3.bias
 (128)" fillcolor=lightblue]
	2585547530464 -> 2585547511024
	2585547511024 [label=AccumulateGrad]
	2585547510880 -> 2585547510832
	2585547530624 [label="act_conv1.weight
 (4, 128, 1, 1)" fillcolor=lightblue]
	2585547530624 -> 2585547510880
	2585547510880 [label=AccumulateGrad]
	2585547510736 -> 2585547510832
	2585547530704 [label="act_conv1.bias
 (4)" fillcolor=lightblue]
	2585547530704 -> 2585547510736
	2585547510736 [label=AccumulateGrad]
	2585547510448 -> 2585547510592
	2585547510448 [label=TBackward0]
	2585547510976 -> 2585547510448
	2585547530784 [label="act_fc1.weight
 (256, 1024)" fillcolor=lightblue]
	2585547530784 -> 2585547510976
	2585547510976 [label=AccumulateGrad]
	2585547510496 -> 2585547532064
	2585547532384 [label="
 (1, 1)" fillcolor=darkolivegreen1]
	2585547510544 [label=TanhBackward0]
	2585547511264 -> 2585547510544
	2585547511264 [label=AddmmBackward0]
	2585547510688 -> 2585547511264
	2585547531424 [label="val_fc2.bias
 (1)" fillcolor=lightblue]
	2585547531424 -> 2585547510688
	2585547510688 [label=AccumulateGrad]
	2585547511072 -> 2585547511264
	2585547511072 [label=ReluBackward0]
	2585547511600 -> 2585547511072
	2585547511600 [label=AddmmBackward0]
	2585547511984 -> 2585547511600
	2585547531264 [label="val_fc1.bias
 (64)" fillcolor=lightblue]
	2585547531264 -> 2585547511984
	2585547511984 [label=AccumulateGrad]
	2585547512032 -> 2585547511600
	2585547512032 [label=ViewBackward0]
	2585547512080 -> 2585547512032
	2585547512080 [label=ReluBackward0]
	2585547512224 -> 2585547512080
	2585547512224 [label=ConvolutionBackward0]
	2585547510928 -> 2585547512224
	2585547512320 -> 2585547512224
	2585547531024 [label="val_conv1.weight
 (2, 128, 1, 1)" fillcolor=lightblue]
	2585547531024 -> 2585547512320
	2585547512320 [label=AccumulateGrad]
	2585547512272 -> 2585547512224
	2585547531104 [label="val_conv1.bias
 (2)" fillcolor=lightblue]
	2585547531104 -> 2585547512272
	2585547512272 [label=AccumulateGrad]
	2585547511888 -> 2585547511600
	2585547511888 [label=TBackward0]
	2585547512128 -> 2585547511888
	2585547531184 [label="val_fc1.weight
 (64, 512)" fillcolor=lightblue]
	2585547531184 -> 2585547512128
	2585547512128 [label=AccumulateGrad]
	2585547510352 -> 2585547511264
	2585547510352 [label=TBackward0]
	2585547512176 -> 2585547510352
	2585547531344 [label="val_fc2.weight
 (1, 64)" fillcolor=lightblue]
	2585547531344 -> 2585547512176
	2585547512176 [label=AccumulateGrad]
	2585547510544 -> 2585547532384
}
